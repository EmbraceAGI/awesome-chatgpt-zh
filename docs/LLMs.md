## LLMs

OpenAI çš„ ChatGPT å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¶æœªå¼€æºï¼Œè¿™éƒ¨åˆ†æ”¶å½•ä¸€äº›æ·±åº¦å­¦ä¹ å¼€æºçš„ LLM ä¾›æ„Ÿå…´è¶£çš„åŒå­¦å­¦ä¹ å‚è€ƒã€‚

### Llama 2 ç³»åˆ— [2023.08.05 æ›´æ–°]

|åç§°|Stars|ç®€ä»‹| å¤‡æ³¨ |
|-------|-------|-------|------|
|[llama 2](https://github.com/facebookresearch/llama) | ![GitHub Repo stars](https://badgen.net/github/stars/facebookresearch/llama) | Inference code for LLaMA models. |llama ç³»åˆ—æ¨¡å‹å®˜æ–¹å¼€æºåœ°å€|
|[codellama](https://github.com/facebookresearch/codellama) | ![GitHub Repo stars](https://badgen.net/github/stars/facebookresearch/codellama) | Inference code for CodeLlama models |ç¼–ç¨‹ä¸“ç”¨ llama ç³»åˆ—æ¨¡å‹å®˜æ–¹å¼€æºåœ°å€|
|[Llama 2ä¸­æ–‡ç¤¾åŒº](https://github.com/FlagAlpha/Llama2-Chinese)| ![GitHub Repo stars](https://badgen.net/github/stars/FlagAlpha/Llama2-Chinese) |-|Llamaä¸­æ–‡ç¤¾åŒº,æœ€å¥½çš„ä¸­æ–‡Llamaå¤§æ¨¡å‹,å®Œå…¨å¼€æºå¯å•†ç”¨|
|[ollama](https://github.com/jmorganca/ollama)| ![GitHub Repo stars](https://badgen.net/github/stars/jmorganca/ollama)| Get up and running with Llama 2 and other large language models locally|æœ¬åœ°è¿è¡Œ llama|
|[Firefly](https://github.com/yangjianxin1/Firefly)| ![GitHub Repo stars](https://badgen.net/github/stars/yangjianxin1/Firefly)|-|Firefly(æµè¤): ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹(å…¨é‡å¾®è°ƒ+QLoRA),æ”¯æŒå¾®è°ƒLlma2ã€Llamaã€Qwenã€Baichuanã€ChatGLM2ã€InternLMã€Ziyaã€Bloom ç­‰å¤§æ¨¡å‹|
|[Azure ChatGPT](https://github.com/microsoft/azurechatgpt)| ![GitHub Repo stars](https://badgen.net/github/stars/microsoft/azurechatgpt) | ğŸ¤– Azure ChatGPT: Private & secure ChatGPT for internal enterprise use ğŸ’¼|-|
|[LLaMA2-Accessory](https://github.com/Alpha-VLLM/LLaMA2-Accessory)| ![GitHub Repo stars](https://badgen.net/github/stars/Alpha-VLLM/LLaMA2-Accessory)| An Open-source Toolkit for LLM Development|-|


### å¤§æ¨¡å‹

|åç§°|Stars|ç®€ä»‹| å¤‡æ³¨ |
|-------|-------|-------|------|
|[Alpaca](https://github.com/tatsu-lab/stanford_alpaca) | ![GitHub Repo stars](https://badgen.net/github/stars/tatsu-lab/stanford_alpaca) | Code and documentation to train Stanford's Alpaca models, and generate the data. |-|
|[WizardLM](https://github.com/nlpxucan/WizardLM) | ![GitHub Repo stars](https://badgen.net/github/stars/nlpxucan/WizardLM) | Family of instruction-following LLMs powered by Evol-Instruct: WizardLM, WizardCoder and WizardMath. |æ•°å­¦èƒ½åŠ›ä¸ ChatGPT ç›¸å·®æ— å‡ çš„å¼€æºå¤§æ¨¡å‹|
|[BELLE](https://github.com/LianjiaTech/BELLE) | ![GitHub Repo stars](https://badgen.net/github/stars/LianjiaTech/BELLE) | A 7B Large Language Model fine-tune by 34B Chinese Character Corpus, based on LLaMA and Alpaca. |-|
|[Bloom](https://github.com/bigscience-workshop/model_card) | ![GitHub Repo stars](https://badgen.net/github/stars/bigscience-workshop/model_card) | BigScience Large Open-science Open-access Multilingual Language Model |-|
|[dolly](https://github.com/databrickslabs/dolly) | ![GitHub Repo stars](https://badgen.net/github/stars/databrickslabs/dolly) | Databricksâ€™ Dolly, a large language model trained on the Databricks Machine Learning Platform |Databricks å‘å¸ƒçš„ Dolly 2.0 å¤§è¯­è¨€æ¨¡å‹ã€‚ä¸šå†…ç¬¬ä¸€ä¸ªå¼€æºã€éµå¾ªæŒ‡ä»¤çš„ LLMï¼Œå®ƒåœ¨é€æ˜ä¸”å…è´¹æä¾›çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œè¯¥æ•°æ®é›†ä¹Ÿæ˜¯å¼€æºçš„ï¼Œå¯ç”¨äºå•†ä¸šç›®çš„ã€‚è¿™æ„å‘³ç€ Dolly 2.0 å¯ç”¨äºæ„å»ºå•†ä¸šåº”ç”¨ç¨‹åºï¼Œæ— éœ€æ”¯ä»˜ API è®¿é—®è´¹ç”¨æˆ–ä¸ç¬¬ä¸‰æ–¹å…±äº«æ•°æ®ã€‚|
|[Falcon 40B](https://huggingface.co/tiiuae/falcon-40b-instruct) | | Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. It is made available under the Apache 2.0 license. |-|
|[FastChat (Vicuna)](https://github.com/lm-sys/FastChat) | ![GitHub Repo stars](https://badgen.net/github/stars/lm-sys/FastChat) | An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and FastChat-T5. |ç»§è‰æ³¥é©¬ï¼ˆAlpacaï¼‰åï¼Œæ–¯å¦ç¦è”æ‰‹CMUã€UCä¼¯å…‹åˆ©ç­‰æœºæ„çš„å­¦è€…å†æ¬¡å‘å¸ƒäº†130äº¿å‚æ•°æ¨¡å‹éª†é©¬ï¼ˆVicunaï¼‰ï¼Œä»…éœ€300ç¾å…ƒå°±èƒ½å®ç°ChatGPT 90%çš„æ€§èƒ½ã€‚|
|[GLM-130B (ChatGLM)](https://github.com/THUDM/GLM-130B) | ![GitHub Repo stars](https://badgen.net/github/stars/THUDM/GLM-130B) | An Open Bilingual Pre-Trained Model (ICLR 2023) |
|[GPT-NeoX](https://github.com/EleutherAI/gpt-neox) | ![GitHub Repo stars](https://badgen.net/github/stars/EleutherAI/gpt-neox) | An implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library. |
|[Luotuo](https://github.com/LC1332/Luotuo-Chinese-LLM) | ![GitHub Repo stars](https://badgen.net/github/stars/LC1332/Luotuo-Chinese-LLM) | An Instruction-following Chinese Language model, LoRA tuning on LLaMA| éª†é©¼ï¼Œä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹å¼€æºé¡¹ç›®ï¼ŒåŒ…å«äº†ä¸€ç³»åˆ—è¯­è¨€æ¨¡å‹ã€‚|
|[minGPT](https://github.com/karpathy/minGPT) |![GitHub Repo stars](https://badgen.net/github/stars/karpathy/minGPT)|A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) trainingã€‚|karpathyå¤§ç¥å‘å¸ƒçš„ä¸€ä¸ª OpenAI GPT(ç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨)è®­ç»ƒçš„æœ€å° PyTorch å®ç°ï¼Œä»£ç ååˆ†ç®€æ´æ˜äº†ï¼Œé€‚åˆç”¨äºåŠ¨æ‰‹å­¦ä¹  GPT æ¨¡å‹ã€‚|
|[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) |![GitHub Repo stars](https://badgen.net/github/stars/THUDM/ChatGLM-6B)|ChatGLM-6B: An Open Bilingual Dialogue Language Model |ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº General Language Model (GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4 é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€ 6GB æ˜¾å­˜ï¼‰ã€‚ ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸­æ–‡é—®ç­”å’Œå¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯çš„åŠ æŒï¼Œ62 äº¿å‚æ•°çš„ ChatGLM-6B å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ã€‚|
|[li-plus/chatglm.cpp](https://github.com/li-plus/chatglm.cpp)|![GitHub Repo stars](https://badgen.net/github/stars/li-plus/chatglm.cpp)|C++ implementation of ChatGLM-6B & ChatGLM2-6B|ChatGLM-6B & ChatGLM2-6B æ¨¡å‹çš„ C++ é«˜æ•ˆå®ç°|
|[Open-Assistant](https://github.com/LAION-AI/Open-Assistant)|![GitHub Repo stars](https://badgen.net/github/stars/LAION-AI/Open-Assistant)|-|çŸ¥å AI æœºæ„ LAION-AI å¼€æºçš„èŠå¤©åŠ©æ‰‹ï¼ŒèŠå¤©èƒ½åŠ›å¾ˆå¼ºï¼Œç›®å‰ä¸­æ–‡èƒ½åŠ›è¾ƒå·®ã€‚|
|[llama.cpp](https://github.com/ggerganov/llama.cpp)|![GitHub Repo stars](https://badgen.net/github/stars/ggerganov/llama.cpp)|-|å®ç°åœ¨MacBookä¸Šè¿è¡Œæ¨¡å‹ã€‚|
|[EasyLM](https://github.com/young-geng/EasyLM#koala)|![GitHub Repo stars](https://badgen.net/github/stars/young-geng/EasyLM)|åœ¨ç¾Šé©¼åŸºç¡€ä¸Šæ”¹è¿›çš„æ–°çš„èŠå¤©æœºå™¨äººè€ƒæ‹‰ã€‚|[ä»‹ç»é¡µ](https://bair.berkeley.edu/blog/2023/04/03/koala/)|
|[FreedomGPT](https://github.com/ohmplatform/FreedomGPT) |![GitHub Repo stars](https://badgen.net/github/stars/ohmplatform/FreedomGPT)|-|è‡ªç”±æ— é™åˆ¶çš„å¯ä»¥åœ¨ windows å’Œ mac ä¸Šæœ¬åœ°è¿è¡Œçš„ GPTï¼ŒåŸºäº Alpaca Lora æ¨¡å‹ã€‚|
|[FinGPT](https://github.com/AI4Finance-Foundation/FinGPT)|![GitHub Repo stars](https://badgen.net/github/stars/AI4Finance-Foundation/FinGPT)|Data-Centric FinGPT. Open-source for open finance! Revolutionize ğŸ”¥ We'll soon release the trained model.|é‡‘èé¢†åŸŸå¤§æ¨¡å‹|
|[baichuan-7B](https://github.com/baichuan-inc/baichuan-7B) |![GitHub Repo stars](https://badgen.net/github/stars/baichuan-inc/baichuan-7B)|A large-scale 7B pretraining language model developed by Baichuan |baichuan-7B æ˜¯ç”±ç™¾å·æ™ºèƒ½å¼€å‘çš„ä¸€ä¸ªå¼€æºå¯å•†ç”¨çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚åŸºäº Transformer ç»“æ„ï¼Œåœ¨å¤§çº¦1.2ä¸‡äº¿ tokens ä¸Šè®­ç»ƒçš„70äº¿å‚æ•°æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º4096ã€‚åœ¨æ ‡å‡†çš„ä¸­æ–‡å’Œè‹±æ–‡æƒå¨ benchmarkï¼ˆC-EVAL/MMLUï¼‰ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚|
|[baichuan-inc/Baichuan-13B](https://github.com/baichuan-inc/Baichuan-13B)|![GitHub Repo stars](https://badgen.net/github/stars/baichuan-inc/Baichuan-13B)|A 13B large language model developed by Baichuan Intelligent Technology|-|
|[open_llama](https://github.com/openlm-research/open_llama) |![GitHub Repo stars](https://badgen.net/github/stars/openlm-research/open_llama)|OpenLLaMA, a permissively licensed open source reproduction of Meta AIâ€™s LLaMA 7B trained on the RedPajama dataset. |OpenLLaMAï¼Œå…è®¸å¼€æºå¤åˆ¶Meta AIçš„LLaMA-7B æ¨¡å‹ï¼Œåœ¨redç¡è¡£æ•°æ®é›†ä¸Šè®­ç»ƒå¾—åˆ°ã€‚|
|[Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)|![GitHub Repo stars](https://badgen.net/github/stars/ymcui/Chinese-LLaMA-Alpaca)|ä¸­æ–‡LLaMAæ¨¡å‹å’Œç»è¿‡æŒ‡ä»¤ç²¾è°ƒçš„Alpacaå¤§æ¨¡å‹ã€‚|-|

### å¤§æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒ
|åç§°|Stars|ç®€ä»‹| å¤‡æ³¨ |
|-------|-------|-------|------|
|[transformers](https://github.com/huggingface/transformers) | ![GitHub Repo stars](https://badgen.net/github/stars/huggingface/transformers) | ğŸ¤— Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX. |HuggingFace ç»å…¸ä¹‹ä½œ, Transformers æ¨¡å‹å¿…ç”¨åº“|
|[peft](https://github.com/huggingface/peft) | ![GitHub Repo stars](https://badgen.net/github/stars/huggingface/peft) | PEFT: State-of-the-art Parameter-Efficient Fine-Tuning. |HuggingFace å‡ºå“â€”â€”PEFT:æœ€å…ˆè¿›çš„å‚æ•°é«˜æ•ˆå¾®è°ƒã€‚|
|[OpenLLM](https://github.com/bentoml/OpenLLM) | ![GitHub Repo stars](https://badgen.net/github/stars/bentoml/OpenLLM) |An open platform for operating large language models (LLMs) in production. Fine-tune, serve, deploy, and monitor any LLMs with ease. |å¾®è°ƒï¼ŒæœåŠ¡ï¼Œéƒ¨ç½²å’Œç›‘æ§æ‰€æœ‰LLMSã€‚ç”¨äºè¿è¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼€æ”¾å¹³å°ã€‚|
|[MLC LLM](https://github.com/mlc-ai/mlc-llm)|![GitHub Repo stars](https://badgen.net/github/stars/mlc-ai/mlc-llm)|Enable everyone to develop, optimize and deploy AI models natively on everyone's devices.|é™ˆå¤©å¥‡å¤§ä½¬åŠ›ä½œâ€”â€”MLC LLMï¼Œåœ¨å„ç±»ç¡¬ä»¶ä¸ŠåŸç”Ÿéƒ¨ç½²ä»»æ„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å¯å°†å¤§æ¨¡å‹åº”ç”¨äºç§»åŠ¨ç«¯ï¼ˆä¾‹å¦‚ iPhoneï¼‰ã€æ¶ˆè´¹çº§ç”µè„‘ç«¯ï¼ˆä¾‹å¦‚ Macï¼‰å’Œ Web æµè§ˆå™¨ã€‚|
|[languagemodels](https://github.com/jncraton/languagemodels)|![GitHub Repo stars](https://badgen.net/github/stars/mlc-ai/mlc-llm)|Explore large language models on any computer with 512MB of RAM.|åœ¨512MB RAMçš„è®¡ç®—æœºä¸Šæ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ä½¿ç”¨|
|[ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning) | ![GitHub Repo stars](https://badgen.net/github/stars/hiyouga/ChatGLM-Efficient-Tuning) | Fine-tuning ChatGLM-6B with PEFT | åŸºäº PEFT çš„é«˜æ•ˆ ChatGLM å¾®è°ƒ|
|[LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) | ![GitHub Repo stars](https://badgen.net/github/stars/hiyouga/LLaMA-Efficient-Tuning) | Fine-tuning LLaMA with PEFT (PT+SFT+RLHF with QLoRA) |æ”¯æŒå¤šç§æ¨¡å‹ LLaMA (7B/13B/33B/65B) ï¼ŒBLOOM & BLOOMZ (560M/1.1B/1.7B/3B/7.1B/176B)ï¼Œbaichuan (7B)ï¼Œæ”¯æŒå¤šç§å¾®è°ƒæ–¹å¼LoRAï¼ŒQLoRA|
|[å¾®è°ƒä¸­æ–‡æ•°æ®é›† COIG](https://github.com/BAAI-Zlab/COIG) | ![GitHub Repo stars](https://badgen.net/github/stars/BAAI-Zlab/COIG) | Chinese Open Instruction Generalist (COIG) project aims to maintain a harmless, helpful, and diverse set of Chinese instruction corpora. |ä¸­æ–‡å¼€æ”¾æ•™å­¦é€šæ‰(COIG)é¡¹ç›®æ—¨åœ¨ç»´æŠ¤ä¸€å¥—æ— å®³ã€æœ‰ç”¨å’Œå¤šæ ·åŒ–çš„ä¸­æ–‡æ•™å­¦è¯­æ–™åº“ã€‚|
|[LLaMA-AdapterğŸš€](https://github.com/ZrrSkywalker/LLaMA-Adapter) | ![GitHub Repo stars](https://badgen.net/github/stars/ZrrSkywalker/LLaMA-Adapter) | - |é«˜æ•ˆå¾®è°ƒä¸€ä¸ªèŠå¤©æœºå™¨äºº|
| [âš¡ Lit-LLaMA](https://github.com/Lightning-AI/lit-llama) | ![GitHub Repo stars](https://badgen.net/github/stars/Lightning-AI/lit-llama) | - |Lightning-AI åŸºäºnanoGPTçš„LLaMAè¯­è¨€æ¨¡å‹çš„å®ç°ã€‚æ”¯æŒé‡åŒ–ï¼ŒLoRAå¾®è°ƒï¼Œé¢„è®­ç»ƒã€‚|
| [IntelÂ® Extension for Transformers](https://github.com/intel/intel-extension-for-transformers) | ![GitHub Repo stars](https://badgen.net/github/stars/intel/intel-extension-for-transformers) |âš¡ Build your chatbot within minutes on your favorite device; offer SOTA compression techniques for LLMs; run LLMs efficiently on Intel Platformsâš¡ |åœ¨Intelå¹³å°ä¸Šé«˜æ•ˆè¿è¡Œllmã€‚|

### æ›´å¤šæ¨¡å‹åˆ—è¡¨
|åç§°|Stars|ç®€ä»‹| å¤‡æ³¨ |
-|-|-|-
|[ğŸ¤– LLMs: awesome-totally-open-chatgpt](https://github.com/nichtdax/awesome-totally-open-chatgpt) |![GitHub Repo stars](https://badgen.net/github/stars/nichtdax/awesome-totally-open-chatgpt)|å¼€æºLLMs æ”¶é›†ã€‚|-|
|[Open LLMs](https://github.com/eugeneyan/open-llms) |![GitHub Repo stars](https://badgen.net/github/stars/eugeneyan/open-llms)|å¼€æºå¯å•†ç”¨çš„å¤§æ¨¡å‹ã€‚|-|
|[Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM) |![GitHub Repo stars](https://badgen.net/github/stars/Hannibal046/Awesome-LLM)|-|å¤§å‹è¯­è¨€æ¨¡å‹çš„è®ºæ–‡åˆ—è¡¨ï¼Œç‰¹åˆ«æ˜¯ä¸ ChatGPTç›¸å…³çš„è®ºæ–‡ï¼Œè¿˜åŒ…å«LLMåŸ¹è®­æ¡†æ¶ã€éƒ¨ç½²LLMçš„å·¥å…·ã€å…³äºLLMçš„è¯¾ç¨‹å’Œæ•™ç¨‹ä»¥åŠæ‰€æœ‰å…¬å¼€å¯ç”¨çš„LLM æƒé‡å’Œ APIã€‚|
|[FindTheChatGPTer](https://github.com/chenking2020/FindTheChatGPTer) |![GitHub Repo stars](https://badgen.net/github/stars/chenking2020/FindTheChatGPTer)|-|æœ¬é¡¹ç›®æ—¨åœ¨æ±‡æ€»é‚£äº›ChatGPTçš„å¼€æºå¹³æ›¿ä»¬ï¼ŒåŒ…æ‹¬æ–‡æœ¬å¤§æ¨¡å‹ã€å¤šæ¨¡æ€å¤§æ¨¡å‹ç­‰|
|[LLMsPracticalGuide](https://github.com/Mooler0410/LLMsPracticalGuide) |![GitHub Repo stars](https://badgen.net/github/stars/Mooler0410/LLMsPracticalGuide)|äºšé©¬é€Šç§‘å­¦å®¶æ¨é–é”‹ç­‰å¤§ä½¬åˆ›å»ºçš„è¯­è¨€å¤§æ¨¡å‹å®è·µæŒ‡å—ï¼Œæ”¶é›†äº†è®¸å¤šç»å…¸çš„è®ºæ–‡ã€ç¤ºä¾‹å’Œå›¾è¡¨ï¼Œå±•ç°äº† GPT è¿™ç±»å¤§æ¨¡å‹çš„å‘å±•å†ç¨‹ç­‰|-|
|[awesome-decentralized-llm](https://github.com/imaurer/awesome-decentralized-llm) |![GitHub Repo stars](https://badgen.net/github/stars/imaurer/awesome-decentralized-llm)|èƒ½åœ¨æœ¬åœ°è¿è¡Œçš„èµ„æº LLMsã€‚|-|
|[OpenChatKit](https://github.com/togethercomputer/OpenChatKit) |![GitHub Repo stars](https://badgen.net/github/stars/togethercomputer/OpenChatKit)|å¼€æºäº†æ•°æ®ã€æ¨¡å‹å’Œæƒé‡ï¼Œä»¥åŠæä¾›è®­ç»ƒï¼Œå¾®è°ƒ LLMs æ•™ç¨‹ã€‚|-|
|[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) |![GitHub Repo stars](https://badgen.net/github/stars/tatsu-lab/stanford_alpaca)|æ¥è‡ªæ–¯å¦ç¦ï¼Œå»ºç«‹å¹¶å…±äº«ä¸€ä¸ªéµå¾ªæŒ‡ä»¤çš„LLaMAæ¨¡å‹ã€‚|-|
|[gpt4all](https://github.com/nomic-ai/gpt4all) |![GitHub Repo stars](https://badgen.net/github/stars/nomic-ai/gpt4all)|åŸºäº LLaMa çš„ LLM åŠ©æ‰‹ï¼Œæä¾›è®­ç»ƒä»£ç ã€æ•°æ®å’Œæ¼”ç¤ºï¼Œè®­ç»ƒä¸€ä¸ªè‡ªå·±çš„ AI åŠ©æ‰‹ã€‚|-|
|[LMFlow](https://github.com/OptimalScale/LMFlow) |![GitHub Repo stars](https://badgen.net/github/stars/OptimalScale/LMFlow)|å…±å»ºå¤§æ¨¡å‹ç¤¾åŒºï¼Œè®©æ¯ä¸ªäººéƒ½è®­å¾—èµ·å¤§æ¨¡å‹ã€‚|-|
|[Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT/blob/main/CN_README.md)|![GitHub Repo stars](https://badgen.net/github/stars/PhoebusSi/Alpaca-CoT)|Alpaca-CoTé¡¹ç›®æ—¨åœ¨æ¢ç©¶å¦‚ä½•æ›´å¥½åœ°é€šè¿‡instruction-tuningçš„æ–¹å¼æ¥è¯±å¯¼LLMå…·å¤‡ç±»ä¼¼ChatGPTçš„äº¤äº’å’Œinstruction-followingèƒ½åŠ›ã€‚|-|
|[OpenFlamingo](https://github.com/mlfoundations/open_flamingo)|![GitHub Repo stars](https://badgen.net/github/stars/mlfoundations/open_flamingo)|OpenFlamingo æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å’Œè®­ç»ƒå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å¼€æºæ¡†æ¶ï¼Œæ˜¯ DeepMind Flamingo æ¨¡å‹çš„å¼€æºç‰ˆæœ¬ï¼Œä¹Ÿæ˜¯ AI ä¸–ç•Œå…³äºå¤§æ¨¡å‹è¿›å±•çš„ä¸€å¤§æ­¥ã€‚|å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å¼€æºæ¡†æ¶ã€‚|
|[LLMs-In-China](https://github.com/wgwang/LLMs-In-China)|![GitHub Repo stars](https://badgen.net/github/stars/wgwang/LLMs-In-China)|ä¸­å›½å¤§æ¨¡å‹|-|
|[Visual OpenLLM](https://github.com/visual-openllm/visual-openllm)|![GitHub Repo stars](https://badgen.net/github/stars/visual-openllm/visual-openllm)|åŸºäº ChatGLM + Visual ChatGPT + Stable Diffusion, ä»¥äº¤äº’æ–¹å¼è¿æ¥ä¸åŒè§†è§‰æ¨¡å‹çš„å¼€æºå·¥å…·ã€‚|-|